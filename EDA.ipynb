{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandjs11/crimedata/blob/main/EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPOncKPV-Eb3"
      },
      "outputs": [],
      "source": [
        "# install PySpark\n",
        "# http://spark.apache.org/docs/latest/api/python/index.html\n",
        "\n",
        "!pip install pyspark==3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvo7FhXa-Ydc"
      },
      "outputs": [],
      "source": [
        "# start spark sessnon and configureation\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "spark = SparkSession.builder.master(\"local[2]\").config(\"spark.driver.memory\", \"15g\")\\\n",
        ".appName(\"Crimedata\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "#create an instance of SQLContext\n",
        "sqlContext = SQLContext(spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnbVpshZ_0wH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUmqIaKiDQjp"
      },
      "outputs": [],
      "source": [
        "# Import libraries and other functions\n",
        "from io import StringIO\n",
        "from collections import namedtuple\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXgOoOfjEWCx"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "\n",
        "# # Replace 'your_zip_file.zip' with the actual name of your uploaded zip file\n",
        "# zip_file_path = '/content/gdrive/MyDrive/dataset/studenthubcrimes.zip'\n",
        "# output_dir = '/content/gdrive/MyDrive/dataset/extracted'\n",
        "\n",
        "# # Create the output directory if it doesn't exist\n",
        "# !mkdir -p \"extracted\"\n",
        "\n",
        "# # Extract the contents of the zip file\n",
        "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(output_dir)\n",
        "\n",
        "# print(\"File extraction complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr7cdslvJDVV"
      },
      "outputs": [],
      "source": [
        "# Define the base directory path where all CSV files are located (ukcrime data)\n",
        "base_directory_path = \"/content/gdrive/MyDrive/dataset/extracted\"\n",
        "\n",
        "# Define the pattern to match all CSV files inside subfolders\n",
        "csv_files_pattern = base_directory_path + \"/*/*street.csv\"\n",
        "\n",
        "# Read all CSV files using the specified pattern\n",
        "CrimeData = spark.read.option(\"header\", \"true\") \\\n",
        "    .option(\"delimiter\", \",\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(csv_files_pattern)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4ndz0K0JXes"
      },
      "outputs": [],
      "source": [
        "CrimeData.take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVWNlseXJZJR"
      },
      "outputs": [],
      "source": [
        "CrimeData.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2smTmn2JeyJ"
      },
      "outputs": [],
      "source": [
        "CrimeData.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NArH-XNoJh5y"
      },
      "outputs": [],
      "source": [
        "total_observations_crime = CrimeData.count()\n",
        "print(\"Total observations in CrimeData:\", total_observations_crime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJxqRHsNNega"
      },
      "outputs": [],
      "source": [
        "missing_values_crime = CrimeData.select\\\n",
        " ([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in CrimeData.columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eC049DVNL3f"
      },
      "outputs": [],
      "source": [
        "print(\"Missing values in CrimeData:\")\n",
        "missing_values_crime.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oXAfuPWGgoNo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heD-3y2qOmug"
      },
      "outputs": [],
      "source": [
        "missing_percentage_crime = missing_values_crime.select(\n",
        "    [(col(c) / total_observations_crime * 100).alias(c) for c in missing_values_crime.columns]\n",
        ")\n",
        "print(\"Missing percentage in CrimeData:\")\n",
        "missing_percentage_crime.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkXgOeCeJ54z"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Filter out rows with missing Latitude\n",
        "CrimeData = CrimeData.filter(col(\"Latitude\").isNotNull())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdHtR_W6KBan"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Count missing values in each column\n",
        "missing_values_count = CrimeData.select(*[sum(col(c).isNull().cast(\"int\")).alias(c)\\\n",
        "                                          for c in CrimeData.columns])\n",
        "\n",
        "# Convert the result to a dictionary for easier display\n",
        "missing_values_dict = missing_values_count.first().asDict()\n",
        "\n",
        "# Display missing values count for each column\n",
        "for column, count in missing_values_dict.items():\n",
        "    print(f\"Column '{column}': {count} missing values\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tn8fxtpKFWW"
      },
      "outputs": [],
      "source": [
        "CrimeData = CrimeData.drop('Crime ID','Falls within','LSOA code','LSOA name',\\\n",
        "                           'Last outcome category','Context')\n",
        "print(f\"{len(CrimeData.columns)} columns in the output dataframe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgtC0OJdKII6"
      },
      "outputs": [],
      "source": [
        "CrimeData.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Count missing values in each column\n",
        "missing_values_count = CrimeData.select(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in CrimeData.columns])\n",
        "\n",
        "# Convert the result to a dictionary for easier display\n",
        "missing_values_dict = missing_values_count.first().asDict()\n",
        "\n",
        "# Display missing values count for each column\n",
        "for column, count in missing_values_dict.items():\n",
        "    print(f\"Column '{column}': {count} missing values\")"
      ],
      "metadata": {
        "id": "fOUjzvPeOIU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9hNxxEFK38h"
      },
      "outputs": [],
      "source": [
        "# tidy up the column names\n",
        "\n",
        "CrimeData = CrimeData.withColumnRenamed('Reported by', 'Reported_by')\n",
        "CrimeData = CrimeData.withColumnRenamed('Crime type', 'Crime_type')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "to_csv=CrimeData\n",
        "output_path = \"CrimeData.csv\"\n",
        "to_csv.write.csv(output_path, header=True)\n",
        "\n",
        "print(\"DataFrame saved to CSV file:\", output_path)"
      ],
      "metadata": {
        "id": "QKIWWAZVyZjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2yubp39UBK8"
      },
      "outputs": [],
      "source": [
        "CrimeData.createTempView(\"CatData\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBkXyWGbUC-m"
      },
      "outputs": [],
      "source": [
        "CrimeCat = spark.sql(\"select distinct Crime_type from CatData\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "df = CrimeData\n",
        "# Count the number of rows with exact location \"On or near\"\n",
        "count_on_or_near = df.filter(col(\"Location\") == \"On or near\").count()\n",
        "\n",
        "# Print the count\n",
        "print(\"Count of rows with exact location 'On or near':\", count_on_or_near)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "k00m0fgwG58J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bamGcyo9UGiy"
      },
      "outputs": [],
      "source": [
        "CrimeCat.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9L52V3C6Adt"
      },
      "outputs": [],
      "source": [
        "BristolCrime=spark.sql(\"select * from CatData where Reported_by='Avon and Somerset Constabulary'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGXm99cPKUz0"
      },
      "outputs": [],
      "source": [
        "LondonCrime=spark.sql(\"select * from CatData where Reported_by='Metropolitan Police Service'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZZTIqvmM0BX"
      },
      "outputs": [],
      "source": [
        "CambridgeCrime=spark.sql(\"select * from CatData where Reported_by='Cambridgeshire Constabulary'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ9gxqbwNAbM"
      },
      "outputs": [],
      "source": [
        "OxfordCrime=spark.sql(\"select * from CatData where Reported_by='Thames Valley Police'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0qP8UjpNBfy"
      },
      "outputs": [],
      "source": [
        "LeedsCrime=spark.sql(\"select * from CatData where Reported_by='West Yorkshire Police'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9Ag82UgY_QK"
      },
      "outputs": [],
      "source": [
        "LondonCrime.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-8_4RS09l40"
      },
      "outputs": [],
      "source": [
        "BristolCrime.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Assuming 'CrimeCat' is your temporary view with crime data\n",
        "# If not, replace 'CrimeCat' with your actual temporary view name\n",
        "# CrimeCat = spark.table(\"CrimeCat\")\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "# pandas_df = CrimeData.toPandas()\n",
        "\n",
        "# Extract year from the date column and add it as a new column\n",
        "pandas_df['Year'] = pd.to_datetime(pandas_df['Month']).dt.year\n",
        "\n",
        "# Group by Year, Reported_by (area), and Crime_type and calculate the count of crimes\n",
        "crime_type_area_year_count = pandas_df.groupby(['Year', 'Reported_by', 'Crime_type']).size().reset_index(name='CrimeCount')\n",
        "\n",
        "# Pivot the data to have crime types as columns\n",
        "pivot_df = crime_type_area_year_count.pivot_table(index=['Year', 'Reported_by'], columns='Crime_type', values='CrimeCount', fill_value=0)\n",
        "\n",
        "# Display the data as a table\n",
        "print(pivot_df)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "aIDUoeiEfcvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Assuming 'CrimeCat' is your temporary view with crime data\n",
        "# If not, replace 'CrimeCat' with your actual temporary view name\n",
        "# CrimeCat = spark.table(\"CrimeCat\")\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "pandas_df = CrimeData.toPandas()\n",
        "\n",
        "# Group by month, city, and crime type and calculate the count of crimes\n",
        "crime_type_month_city_count = pandas_df.groupby(['Month', 'Reported_by', 'Crime_type']).size().reset_index(name='CrimeCount')\n",
        "\n",
        "# Pivot the data to have crime types as columns\n",
        "pivot_df = crime_type_month_city_count.pivot_table(index=['Month', 'Reported_by'], columns='Crime_type', values='CrimeCount', fill_value=0)\n",
        "\n",
        "# Display the data as a table\n",
        "print(pivot_df)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "X0iFgy1UbURI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "# CrimeData = spark.read.option(\"header\", \"true\").csv(\"path_to_your_dataset.csv\")\n",
        "df=CrimeData\n",
        "# Filter the data for the year 2021 and \"Metropolitan Police Service\"\n",
        "filtered_data = df.filter((year(col(\"Month\")) == 2022) & (col(\"Reported_by\") == \"Metropolitan Police Service\"))\n",
        "\n",
        "# Group by \"Reported by\" and \"Month\" and count the number of crimes\n",
        "crime_counts = filtered_data.groupBy(\"Reported_by\", \"Month\").count()\n",
        "\n",
        "# Show the resulting counts\n",
        "crime_counts.orderBy(\"count\").show(12)\n"
      ],
      "metadata": {
        "id": "da9NLrwhQQo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "# CrimeData = spark.read.option(\"header\", \"true\").csv(\"path_to_your_dataset.csv\")\n",
        "df=CrimeData\n",
        "# Filter the data for the year 2021 and \"Metropolitan Police Service\"\n",
        "filtered_data = df.filter((year(col(\"Month\")) == 2022) & (col(\"Reported_by\") == \"Cambridgeshire Constabulary\"))\n",
        "\n",
        "# Group by \"Reported by\" and \"Month\" and count the number of crimes\n",
        "crime_counts = filtered_data.groupBy(\"Reported_by\", \"Month\").count()\n",
        "\n",
        "# Show the resulting counts\n",
        "crime_counts.orderBy(\"count\").show(12)\n"
      ],
      "metadata": {
        "id": "EQ22qvYqgfaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "# CrimeData = spark.read.option(\"header\", \"true\").csv(\"path_to_your_dataset.csv\")\n",
        "df=CrimeData\n",
        "# Filter the data for the year 2021 and \"Metropolitan Police Service\"\n",
        "filtered_data = df.filter((year(col(\"Month\")) == 2022) & (col(\"Reported_by\") == \"Avon and Somerset Constabulary\"))\n",
        "\n",
        "# Group by \"Reported by\" and \"Month\" and count the number of crimes\n",
        "crime_counts = filtered_data.groupBy(\"Reported_by\", \"Month\").count()\n",
        "\n",
        "# Show the resulting counts\n",
        "crime_counts.orderBy(\"count\").show(12)\n"
      ],
      "metadata": {
        "id": "esoz0izTg0KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "# CrimeData = spark.read.option(\"header\", \"true\").csv(\"path_to_your_dataset.csv\")\n",
        "df=CrimeData\n",
        "# Filter the data for the year 2021 and \"Metropolitan Police Service\"\n",
        "filtered_data = df.filter((year(col(\"Month\")) == 2022) & (col(\"Reported_by\") == \"Thames Valley Police\"))\n",
        "\n",
        "# Group by \"Reported by\" and \"Month\" and count the number of crimes\n",
        "crime_counts = filtered_data.groupBy(\"Reported_by\", \"Month\").count()\n",
        "\n",
        "# Show the resulting counts\n",
        "crime_counts.orderBy(\"count\").show(12)\n"
      ],
      "metadata": {
        "id": "JOpl4tT_hBrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "# CrimeData = spark.read.option(\"header\", \"true\").csv(\"path_to_your_dataset.csv\")\n",
        "df=CrimeData\n",
        "# Filter the data for the year 2021 and \"Metropolitan Police Service\"\n",
        "filtered_data = df.filter((year(col(\"Month\")) == 2022) & (col(\"Reported_by\") == \"West Yorkshire Police\"))\n",
        "\n",
        "# Group by \"Reported by\" and \"Month\" and count the number of crimes\n",
        "crime_counts = filtered_data.groupBy(\"Reported_by\", \"Month\").count()\n",
        "\n",
        "# Show the resulting counts\n",
        "crime_counts.orderBy(\"count\").show(12)\n"
      ],
      "metadata": {
        "id": "8tmbpzPbhKH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vl7yHPz9L2R"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "# CrimeData = spark.read.option(\"header\", \"true\").csv(\"path_to_your_dataset.csv\")\n",
        "\n",
        "# Assuming 'CrimeData' is your DataFrame\n",
        "# If not, replace 'CrimeData' with your actual DataFrame name\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "# pandas_df = CrimeData.toPandas()\n",
        "\n",
        "# Filter the data for the years 2021 and 2022\n",
        "filtered_data = pandas_df[pandas_df['Month'].str.startswith(('2021', '2022'))]\n",
        "\n",
        "# Group by month, reported by, and crime type and calculate the count of crimes\n",
        "crime_type_month_city_count = filtered_data.groupby(['Month', 'Reported_by', 'Crime_type']).size().reset_index(name='CrimeCount')\n",
        "\n",
        "# Pivot the data to have crime types as columns\n",
        "pivot_df = crime_type_month_city_count.pivot_table(index=['Month', 'Reported_by'], columns='Crime_type', values='CrimeCount', fill_value=0)\n",
        "\n",
        "# Plot the data\n",
        "for city in pivot_df.index.get_level_values('Reported_by').unique():\n",
        "    city_data = pivot_df.loc[pivot_df.index.get_level_values('Reported_by') == city]\n",
        "\n",
        "    plt.figure(figsize=(20, 8))\n",
        "    city_data.plot(kind='bar', ax=plt.gca())\n",
        "\n",
        "    plt.title(f'Crime Types Distribution Each Month in {city}')\n",
        "    plt.xlabel('Month')\n",
        "    plt.ylabel('Crime Count')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xticks(rotation=45, ha=\"right\")  # Rotating x-axis labels for readability\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "df=CrimeData\n",
        "# Filter the data for the year 2021 and \"Metropolitan Police Service\"\n",
        "data_2021 = df.filter(\n",
        "    (year(col(\"Month\")) == 2021) &\n",
        "    (col(\"Reported by\") == \"West Yorkshire Police\")\n",
        ")\n",
        "\n",
        "# Filter the data for the year 2022 and \"Metropolitan Police Service\"\n",
        "data_2022 = df.filter(\n",
        "    (year(col(\"Month\")) == 2022) &\n",
        "    (col(\"Reported by\") == \"West Yorkshire Police\")\n",
        ")\n",
        "\n",
        "# Group by \"Crime type\" and count the occurrences for 2021\n",
        "crime_type_counts_2021 = data_2021.groupBy(\"Crime_type\").count()\n",
        "\n",
        "# Group by \"Crime type\" and count the occurrences for 2022\n",
        "crime_type_counts_2022 = data_2022.groupBy(\"Crime_type\").count()\n",
        "\n",
        "# Show the resulting counts for 2021\n",
        "print(\"Crime Type Counts for 2021:\")\n",
        "crime_type_counts_2021.show(truncate=False)\n",
        "\n",
        "# Show the resulting counts for 2022\n",
        "print(\"Crime Type Counts for 2022:\")\n",
        "crime_type_counts_2022.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "YjBeh0uNq1R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZhUNXtnEl3f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "input_path=\"/content/CrimeData.csv\"\n",
        "crime_data= spark.read.csv(input_path, header=True, inferSchema=True)\n",
        "crime_data=crime_data.toPandas()\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "crime_data['Month'] = pd.to_datetime(crime_data['Month'])\n",
        "\n",
        "# Filter data for the year 2021 and for \"Violence and sexual offences\" crime type\n",
        "violence_sexual_offences_2021 = crime_data[\n",
        "    (crime_data['Month'].dt.year == 2021) &\n",
        "    (crime_data['Crime_type'] == 'Violence and sexual offences')&\n",
        "    (crime_data['Reported_by']=='Metropolitan Police Service')\n",
        "]\n",
        "\n",
        "# Count the total number of reported incidents\n",
        "total_incidents_2021 = len(violence_sexual_offences_2021)\n",
        "\n",
        "print(\"Total number of Violence and Sexual Offences reported in 2021:\", total_incidents_2021)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset into a DataFrame (assuming the variable 'df' contains your data)\n",
        "df = CrimeData\n",
        "\n",
        "# Filter for 'Violence and sexual offences'\n",
        "violence_sexual_offences = df.filter(col(\"Crime_type\") == \"Violence and sexual offences\")\n",
        "\n",
        "# Group by 'Location' and count the occurrences\n",
        "location_counts = violence_sexual_offences.groupBy(\"Location\").count()\n",
        "\n",
        "# Find the location with the maximum count\n",
        "max_location = location_counts.orderBy(col(\"count\").desc()).first()\n",
        "\n",
        "print(\"Location with Maximum Count of Violence and Sexual Offences:\")\n",
        "print(\"Location:\", max_location[\"Location\"])\n",
        "print(\"Count:\", max_location[\"count\"])\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "wfUIKAB8wl5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset into a DataFrame (assuming the variable 'df' contains your data)\n",
        "df = CrimeData\n",
        "\n",
        "# Filter for rows where Location is exactly \"On or near\"\n",
        "on_or_near_locations = df.filter(col(\"Location\") == \"On or near \")\n",
        "\n",
        "# Display the top 10 rows\n",
        "top_10_on_or_near_locations = on_or_near_locations.limit(10)\n",
        "top_10_on_or_near_locations.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "A97xUvG8UqOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJgRWFoU6PIa"
      },
      "source": [
        "Violence and sexual offences 2021"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Load data from CSV file into a Spark DataFrame (replace with your input path)\n",
        "\n",
        "crime_data = CrimeData\n",
        "\n",
        "# Filter relevant columns\n",
        "filtered_data = crime_data.select(\"Reported_by\", \"Location\", \"Crime_type\")\n",
        "\n",
        "# Group by \"Reported_by\" and \"Location\", and count occurrences\n",
        "location_counts = filtered_data.groupBy(\"Reported_by\", \"Location\").count()\n",
        "\n",
        "# Create a temporary view for the location counts\n",
        "location_counts.createOrReplaceTempView(\"LocationCounts\")\n",
        "\n",
        "# Get unique \"Reported by\" values\n",
        "reported_by_list = location_counts.select(\"Reported_by\").distinct().collect()\n",
        "\n",
        "# Loop through each \"Reported by\" and get the top 10 locations\n",
        "for reported_by_row in reported_by_list:\n",
        "    reported_by = reported_by_row.Reported_by\n",
        "\n",
        "    top_10_locations = spark.sql(f\"\"\"\n",
        "        SELECT Location, count\n",
        "        FROM LocationCounts\n",
        "        WHERE Reported_by = '{reported_by}'\n",
        "        ORDER BY count DESC\n",
        "    \"\"\")\n",
        "\n",
        "    # Show the results for each \"Reported by\"\n",
        "    print(f\"Top 10 Locations for {reported_by}:\")\n",
        "    top_10_locations.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "iF9txfEqrKnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "df = CrimeData\n",
        "\n",
        "# Get unique \"Reported by\" values\n",
        "reported_by_list = df.select(\"Reported_by\").dropDuplicates().collect()\n",
        "\n",
        "# Filter the data for each \"Reported by\" and create a pie chart\n",
        "for reported_by_row in reported_by_list:\n",
        "    reported_by = reported_by_row.Reported_by\n",
        "\n",
        "    filtered_data = df.filter(\n",
        "        (col(\"Crime type\") == \"Violence and sexual offences\") &\n",
        "        (col(\"Reported by\") == reported_by) &\n",
        "        (~col(\"Location\").isin([\"on or near \", \"On or near \"])))\n",
        "\n",
        "    # Group by \"Location\" and count the occurrences\n",
        "    location_counts = filtered_data.groupBy(\"Location\").count()\n",
        "\n",
        "    top_10_locations = location_counts.orderBy(col(\"count\").desc()).limit(10)\n",
        "    top_10_locations.show()\n",
        "\n",
        "    # Convert Spark DataFrame to Pandas DataFrame\n",
        "    pandas_df = top_10_locations.toPandas()\n",
        "\n",
        "    # Create a pie chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.pie(pandas_df[\"count\"], labels=pandas_df[\"Location\"], autopct=\"%1.1f%%\", startangle=140)\n",
        "    plt.axis(\"equal\")  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "    plt.title(f\"Locations of Violence and Sexual Offences ({reported_by})\")\n",
        "    plt.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "5kHoC8DBwnL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the dataset\n",
        "df = CrimeData\n",
        "\n",
        "# Get unique \"Reported by\" values\n",
        "reported_by_list = df.select(\"Reported_by\").dropDuplicates().collect()\n",
        "\n",
        "# Create a map centered at a specified location\n",
        "m = folium.Map(location=[51.5074, -0.1278], zoom_start=10, tiles='cartodbpositron')\n",
        "\n",
        "# Loop through each \"Reported by\" value\n",
        "for reported_by_row in reported_by_list:\n",
        "    reported_by = reported_by_row.Reported_by\n",
        "\n",
        "    filtered_data = df.filter(\n",
        "        (col(\"Crime type\") == \"Violence and sexual offences\") &\n",
        "        (col(\"Reported by\") == reported_by) &\n",
        "        (~col(\"Location\").isin([\"on or near \", \"On or near \"])))\n",
        "\n",
        "    # Group by latitude and longitude and count the occurrences\n",
        "    location_counts = filtered_data.groupBy(\"Latitude\", \"Longitude\").count()\n",
        "\n",
        "    top_10_locations = location_counts.orderBy(col(\"count\").desc()).limit(10)\n",
        "    # top_10_locations.show()\n",
        "\n",
        "    # Convert Spark DataFrame to Pandas DataFrame\n",
        "    pandas_df = top_10_locations.toPandas()\n",
        "\n",
        "    # Convert location data to a list of tuples with latitude, longitude, and count\n",
        "    heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in pandas_df.iterrows()]\n",
        "\n",
        "    # Create a HeatMap layer\n",
        "    HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m.save(\"heatmap.html\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "u3T6Gb3u0HBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.save(\"heatmap.html\")"
      ],
      "metadata": {
        "id": "QVVwflqGkpnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fdIX059PxNi"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Location, Month)\n",
        "location_data = df.select(\"Longitude\", \"Latitude\", \"Crime_type\", \"Location\", \"Month\")\n",
        "\n",
        "# Convert the \"Month\" column to a date format\n",
        "location_data = location_data.withColumn(\"Month\", col(\"Month\").cast(\"date\"))\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data.filter(\n",
        "    (col(\"Crime_type\") == \"Violence and sexual offences\") &\n",
        "    (col(\"Month\").between(\"2021-01\", \"2021-12\"))\n",
        ")\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.5074, -0.1278], zoom_start=10, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row.Latitude, row.Longitude, 1) for row in violence_sexual_offences_2021.collect()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMwK9lSD6Va8"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Violence and sexual offences') &\n",
        "    (location_data['Month'].dt.year == 2021)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXpB9kEM6-qc"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Violence and sexual offences') &\n",
        "    (location_data['Month'].dt.year == 2023)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwLzbirR7np7"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Bicycle theft') &\n",
        "    (location_data['Month'].dt.year == 2021)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p8O7T9V78B6"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Bicycle theft') &\n",
        "    (location_data['Month'].dt.year == 2022)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eh0nkIqe8GED"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Bicycle theft') &\n",
        "    (location_data['Month'].dt.year == 2023)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd3ypaA88LRE"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Public order') &\n",
        "    (location_data['Month'].dt.year == 2021)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S162APdy81qh"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Public order') &\n",
        "    (location_data['Month'].dt.year == 2022)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw-Ue_Ma89GK"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Public order') &\n",
        "    (location_data['Month'].dt.year == 2023)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0nWsR6T9BnE"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Drugs') &\n",
        "    (location_data['Month'].dt.year == 2021)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD9pmMzm9SpB"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Drugs') &\n",
        "    (location_data['Month'].dt.year == 2022)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcjqgS5X9aPY"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Drugs') &\n",
        "    (location_data['Month'].dt.year == 2023)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6ITvsJv9h0D"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Other crime') &\n",
        "    (location_data['Month'].dt.year == 2021)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieJnlCFT9xih"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Other crime') &\n",
        "    (location_data['Month'].dt.year == 2022)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK9gcKta91iz"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Other crime') &\n",
        "    (location_data['Month'].dt.year == 2023)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QgBreKn96KF"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Robbery') &\n",
        "    (location_data['Month'].dt.year == 2021)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqdRVvId-G5w"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Robbery') &\n",
        "    (location_data['Month'].dt.year == 2022)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0peu4eV-OEx"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Robbery') &\n",
        "    (location_data['Month'].dt.year == 2023)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmc3y1a6-geg"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Criminal damage and arson') &\n",
        "    (location_data['Month'].dt.year == 2021)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B2iSthM-zo2"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Criminal damage and arson') &\n",
        "    (location_data['Month'].dt.year == 2022)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUoph8P2-5OT"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Criminal damage and arson') &\n",
        "    (location_data['Month'].dt.year == 2023)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghRDlcGZ-_IK"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Theft from the person') &\n",
        "    (location_data['Month'].dt.year == 2021)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vuPT9RvQD4p"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Theft from the person') &\n",
        "    (location_data['Month'].dt.year == 2022)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-RkPThnQ0fC"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"CrimeDataAnalysis\").getOrCreate()\n",
        "\n",
        "# Filter relevant columns (Longitude, Latitude, Crime_type, Month)\n",
        "location_data = pandas_df[['Longitude', 'Latitude', 'Crime_type', 'Month']]\n",
        "\n",
        "# Convert the \"Month\" column to a datetime format\n",
        "location_data['Month'] = pd.to_datetime(location_data['Month'])\n",
        "\n",
        "# Filter only 'Violence and sexual offences' incidents in the year 2021\n",
        "violence_sexual_offences_2021 = location_data[\n",
        "    (location_data['Crime_type'] == 'Theft from the person') &\n",
        "    (location_data['Month'].dt.year == 2023)\n",
        "]\n",
        "\n",
        "# Create a map centered at the specified location\n",
        "m = folium.Map(location=[51.4203, 0.0705], zoom_start=12, tiles='cartodbpositron')\n",
        "\n",
        "# Convert location data to a list of tuples with latitude, longitude, and count\n",
        "heat_data = [(row['Latitude'], row['Longitude'], 1) for index, row in violence_sexual_offences_2021.iterrows()]\n",
        "\n",
        "# Create a HeatMap layer\n",
        "HeatMap(data=heat_data, radius=15).add_to(m)\n",
        "\n",
        "# Display the map\n",
        "m\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1muMYd2cLym48JG5RT3iGFYdM22wrHvXJ",
      "authorship_tag": "ABX9TyMJC+ICu/YBy16fjNAW5uqd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}