{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKCSV5XP8FxG/uGnfACcNN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandjs11/crimedata/blob/main/GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko02904_bMZM"
      },
      "outputs": [],
      "source": [
        "# install PySpark\n",
        "# http://spark.apache.org/docs/latest/api/python/index.html\n",
        "\n",
        "!pip install pyspark==3.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start spark sessnon and configureation\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "spark = SparkSession.builder.master(\"local[2]\").config(\"spark.driver.memory\", \"15g\").appName(\"Crimedata\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "#create an instance of SQLContext\n",
        "sqlContext = SQLContext(spark)"
      ],
      "metadata": {
        "id": "dilqqbuTbPxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "TO2e0vfobR9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and other functions\n",
        "from io import StringIO\n",
        "from collections import namedtuple\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "pgcT0KJgbTxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base directory path where all CSV files are located (ukcrime data)\n",
        "base_directory_path = \"/content/gdrive/MyDrive/dataset/extracted\"\n",
        "\n",
        "# Define the pattern to match all CSV files inside subfolders\n",
        "csv_files_pattern = base_directory_path + \"/*/*street.csv\"\n",
        "\n",
        "# Read all CSV files using the specified pattern\n",
        "CrimeData = spark.read.option(\"header\", \"true\") \\\n",
        "    .option(\"delimiter\", \",\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(csv_files_pattern)"
      ],
      "metadata": {
        "id": "g3SVYLDxbVzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Filter out rows with missing Latitude\n",
        "CrimeData = CrimeData.filter(col(\"Latitude\").isNotNull())"
      ],
      "metadata": {
        "id": "fIKWMn9vbXra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CrimeData = CrimeData.drop('Crime ID','Falls within','LSOA code','LSOA name','Last outcome category','Context')\n",
        "print(f\"{len(CrimeData.columns)} columns in the output dataframe\")"
      ],
      "metadata": {
        "id": "QcoOGK81bZc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tidy up the column names\n",
        "\n",
        "CrimeData = CrimeData.withColumnRenamed('Reported by', 'Reported_by')\n",
        "CrimeData = CrimeData.withColumnRenamed('Crime type', 'Crime_type')"
      ],
      "metadata": {
        "id": "8BMFNRihbbQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start Neural Networks"
      ],
      "metadata": {
        "id": "zr4yo2wibgR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"GRUForecasting\").getOrCreate()\n",
        "\n",
        "# Assuming 'CrimeData' is your Spark DataFrame\n",
        "# If not, replace 'CrimeData' with your actual DataFrame name\n",
        "# CrimeData = spark.table(\"CrimeData\")\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "# pandas_df = CrimeData.toPandas()\n",
        "\n",
        "# Filter data for \"Violence and sexual offences\" and all 'Reported_by'\n",
        "filtered_data = pandas_df[pandas_df['Crime_type'] == 'Violence and sexual offences']\n",
        "\n",
        "# Get a list of unique 'Reported_by' values\n",
        "reported_by_values = filtered_data['Reported_by'].unique()\n",
        "\n",
        "# Set the time window for training data\n",
        "train_window = 12  # Use the last 12 months to train the model\n",
        "\n",
        "# Perform GRU forecasting for each 'Reported_by' value\n",
        "for reported_by in reported_by_values:\n",
        "    reported_by_data = filtered_data[filtered_data['Reported_by'] == reported_by]\n",
        "\n",
        "    # Group by month and calculate the sum of crimes\n",
        "    reported_by_monthly = reported_by_data.groupby('Month').size().reset_index(name='Count')\n",
        "\n",
        "    # Convert the 'Month' column to datetime\n",
        "    reported_by_monthly['Month'] = pd.to_datetime(reported_by_monthly['Month'], format='%Y-%m')\n",
        "    reported_by_monthly.set_index('Month', inplace=True)\n",
        "\n",
        "    # Normalize data\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(reported_by_monthly[['Count']])\n",
        "\n",
        "    # Prepare data for training\n",
        "    X, y = [], []\n",
        "    for i in range(train_window, len(scaled_data)):\n",
        "        X.append(scaled_data[i - train_window:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    # Reshape input to be [samples, time steps, features]\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "    # Create and train the GRU model\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units=50, activation='relu', input_shape=(X.shape[1], 1)))\n",
        "    model.add(Dense(units=1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X, y, epochs=50, batch_size=16)\n",
        "\n",
        "    # Forecast the next 6 months\n",
        "    forecast_steps = 6\n",
        "    last_window = scaled_data[-train_window:, 0]\n",
        "    forecast = []\n",
        "    for i in range(forecast_steps):\n",
        "        input_data = last_window[-train_window:].reshape(1, train_window, 1)\n",
        "        predicted_value = model.predict(input_data)\n",
        "        forecast.append(predicted_value)\n",
        "        last_window = np.append(last_window, predicted_value)\n",
        "\n",
        "    forecast = np.array(forecast).flatten()\n",
        "    forecast = scaler.inverse_transform(forecast.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Create forecast date range\n",
        "    forecast_dates = pd.date_range(start=reported_by_monthly.index[-1], periods=forecast_steps + 1, freq='M')[1:]\n",
        "\n",
        "    # Plot the original data and the forecast\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(reported_by_monthly.index, reported_by_monthly['Count'], label='Original Data')\n",
        "    plt.plot(forecast_dates, forecast, label='Forecast', linestyle='dashed')\n",
        "    plt.title(f'GRU Forecasting for Crime Type: Violence and sexual offences - Reported by: {reported_by}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "ME4WxnYrenbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"GRUForecasting\").getOrCreate()\n",
        "\n",
        "# Assuming 'CrimeData' is your Spark DataFrame\n",
        "# If not, replace 'CrimeData' with your actual DataFrame name\n",
        "# CrimeData = spark.table(\"CrimeData\")\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "# pandas_df = CrimeData.toPandas()\n",
        "\n",
        "# Filter data for \"Violence and sexual offences\" and all 'Reported_by'\n",
        "filtered_data = pandas_df[pandas_df['Crime_type'] == 'Violence and sexual offences']\n",
        "\n",
        "# Get a list of unique 'Reported_by' values\n",
        "reported_by_values = filtered_data['Reported_by'].unique()\n",
        "\n",
        "# Set the time window for training data\n",
        "train_window = 12  # Use the last 12 months to train the model\n",
        "\n",
        "# Initialize a list to store forecast results\n",
        "forecast_results = []\n",
        "\n",
        "# Perform GRU forecasting for each 'Reported_by' value\n",
        "for reported_by in reported_by_values:\n",
        "    reported_by_data = filtered_data[filtered_data['Reported_by'] == reported_by]\n",
        "\n",
        "    # Group by month and calculate the sum of crimes\n",
        "    reported_by_monthly = reported_by_data.groupby('Month').size().reset_index(name='Count')\n",
        "\n",
        "    # Convert the 'Month' column to datetime\n",
        "    reported_by_monthly['Month'] = pd.to_datetime(reported_by_monthly['Month'], format='%Y-%m')\n",
        "    reported_by_monthly.set_index('Month', inplace=True)\n",
        "\n",
        "    # Normalize data\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(reported_by_monthly[['Count']])\n",
        "\n",
        "    # Prepare data for training\n",
        "    X, y = [], []\n",
        "    for i in range(train_window, len(scaled_data)):\n",
        "        X.append(scaled_data[i - train_window:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    # Reshape input to be [samples, time steps, features]\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "    # Create and train the GRU model\n",
        "    model = Sequential()\n",
        "    model.add(GRU(units=50, activation='relu', input_shape=(X.shape[1], 1)))\n",
        "    model.add(Dense(units=1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X, y, epochs=50, batch_size=16)\n",
        "\n",
        "    # Forecast the next 6 months\n",
        "    forecast_steps = 6\n",
        "    last_window = scaled_data[-train_window:, 0]\n",
        "    forecast = []\n",
        "    for i in range(forecast_steps):\n",
        "        input_data = last_window[-train_window:].reshape(1, train_window, 1)\n",
        "        predicted_value = model.predict(input_data)\n",
        "        forecast.append(predicted_value)\n",
        "        last_window = np.append(last_window, predicted_value)\n",
        "\n",
        "    forecast = np.array(forecast).flatten()\n",
        "    forecast = scaler.inverse_transform(forecast.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Create forecast date range\n",
        "    forecast_dates = pd.date_range(start=reported_by_monthly.index[-1], periods=forecast_steps + 1, freq='M')[1:]\n",
        "\n",
        "    # # Plot the original data and the forecast\n",
        "    # plt.figure(figsize=(10, 6))\n",
        "    # plt.plot(reported_by_monthly.index, reported_by_monthly['Count'], label='Original Data')\n",
        "    # plt.plot(forecast_dates, forecast, label='Forecast', linestyle='dashed')\n",
        "    # plt.title(f'GRU Forecasting for Crime Type: Violence and sexual offences - Reported by: {reported_by}')\n",
        "    # plt.xlabel('Date')\n",
        "    # plt.ylabel('Count')\n",
        "    # plt.legend()\n",
        "    # plt.show()\n",
        "\n",
        "    # Calculate MAE and MAPE\n",
        "    actual_values = reported_by_monthly['Count'][-forecast_steps:].to_numpy()\n",
        "    mae = np.abs(forecast - actual_values).mean()\n",
        "    mape = (np.abs(forecast - actual_values) / actual_values).mean() * 100\n",
        "\n",
        "    # Append results to the forecast_results list\n",
        "    forecast_results.append({\n",
        "        'Reported_by': reported_by,\n",
        "        'Forecast': forecast,\n",
        "        'MAE': mae,\n",
        "        'MAPE': mape\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the forecast results\n",
        "forecast_df = pd.DataFrame(forecast_results)\n",
        "\n",
        "# Display the tabular forecast results using tabulate\n",
        "table = tabulate(forecast_df, headers='keys', tablefmt='psql')\n",
        "print(table)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "k7QpumJIiPYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tabulate import tabulate\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"GRUForecasting\").getOrCreate()\n",
        "\n",
        "# Assuming 'CrimeData' is your Spark DataFrame\n",
        "# If not, replace 'CrimeData' with your actual DataFrame name\n",
        "# CrimeData = spark.table(\"CrimeData\")\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "pandas_df = CrimeData.toPandas()\n",
        "\n",
        "# Filter data for \"Avon and Somerset Constabulary\"\n",
        "metropolitan_data = pandas_df[pandas_df['Reported_by'] == 'Metropolitan Police Service']\n",
        "\n",
        "# Get a list of unique crime types\n",
        "crime_types = metropolitan_data['Crime_type'].unique()\n",
        "\n",
        "# Create a list to store the forecasted results\n",
        "forecast_results = []\n",
        "\n",
        "# Create a list to store the accuracy metrics\n",
        "accuracy_results = []\n",
        "\n",
        "# Define hyperparameters\n",
        "input_sequence_length = 12  # Number of months to use as input sequence\n",
        "output_sequence_length = 6  # Number of months to forecast\n",
        "hidden_units = 64\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "\n",
        "# Perform GRU forecasting for each crime type\n",
        "for crime_type in crime_types:\n",
        "    crime_type_data = metropolitan_data[metropolitan_data['Crime_type'] == crime_type]\n",
        "\n",
        "    # Group by month and calculate the sum of crimes\n",
        "    crime_type_monthly = crime_type_data.groupby('Month').size().reset_index(name='Count')\n",
        "\n",
        "    # Convert the 'Month' column to datetime\n",
        "    crime_type_monthly['Month'] = pd.to_datetime(crime_type_monthly['Month'], format='%Y-%m')\n",
        "    crime_type_monthly.set_index('Month', inplace=True)\n",
        "\n",
        "    # Normalize the data\n",
        "    scaler = MinMaxScaler()\n",
        "    crime_type_monthly['Count'] = scaler.fit_transform(crime_type_monthly[['Count']])\n",
        "\n",
        "    # Prepare the data for training\n",
        "    data = np.array(crime_type_monthly['Count'])\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(len(data) - input_sequence_length - output_sequence_length + 1):\n",
        "        X.append(data[i:i + input_sequence_length])\n",
        "        y.append(data[i + input_sequence_length:i + input_sequence_length + output_sequence_length])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Build the GRU model\n",
        "    model = Sequential([\n",
        "        GRU(hidden_units, input_shape=(input_sequence_length, 1)),\n",
        "        Dense(output_sequence_length)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(), loss='mse')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X, y, batch_size=batch_size, epochs=epochs, verbose=0)\n",
        "\n",
        "    # Forecast the next 6 months\n",
        "    last_sequence = data[-input_sequence_length:]\n",
        "    forecast = []\n",
        "\n",
        "    for _ in range(output_sequence_length):\n",
        "        input_data = np.array([last_sequence])\n",
        "        input_data = np.reshape(input_data, (input_data.shape[0], input_data.shape[1], 1))\n",
        "        predicted = model.predict(input_data)\n",
        "        forecast.append(predicted[0, -1])\n",
        "        last_sequence = np.append(last_sequence[1:], predicted[0, -1])\n",
        "\n",
        "    forecast = np.array(forecast)\n",
        "    forecast = scaler.inverse_transform(forecast.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Append forecasted values to the list\n",
        "    forecast_results.append({'Crime_Type': crime_type, 'Forecast': list(forecast)})\n",
        "\n",
        "    # Extract the actual values for the forecast period\n",
        "    actual_values = crime_type_monthly['Count'].values[-output_sequence_length:]\n",
        "\n",
        "    # Calculate Mean Absolute Error (MAE)\n",
        "    mae = np.mean(np.abs(actual_values - forecast))\n",
        "\n",
        "    # Calculate Mean Absolute Percentage Error (MAPE)\n",
        "    mape = np.mean(np.abs((actual_values - forecast) / actual_values)) * 100\n",
        "\n",
        "    # Append accuracy metrics to the list\n",
        "    accuracy_results.append({'Crime_Type': crime_type, 'MAE': mae, 'MAPE': mape})\n",
        "\n",
        "# Convert the list of forecast results to a Pandas DataFrame\n",
        "forecast_df = pd.DataFrame(forecast_results)\n",
        "\n",
        "# Convert the list of accuracy results to a Pandas DataFrame\n",
        "accuracy_df = pd.DataFrame(accuracy_results)\n",
        "\n",
        "# Display the forecast results and accuracy metrics in tabular form\n",
        "formatted_forecast_table = tabulate(forecast_df, headers='keys', tablefmt='pretty')\n",
        "formatted_accuracy_table = tabulate(accuracy_df, headers='keys', tablefmt='pretty')\n",
        "\n",
        "print(\"Forecast Results:\")\n",
        "print(formatted_forecast_table)\n",
        "\n",
        "print(\"\\nAccuracy Metrics:\")\n",
        "print(formatted_accuracy_table)\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "F2wzr9f8bmVg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}